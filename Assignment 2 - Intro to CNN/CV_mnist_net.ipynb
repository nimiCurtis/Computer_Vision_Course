{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Gbl4asdQIFCc"},"source":["# MNIST Digits Classification using Neural Networks\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ykpbn7glGgMn"},"source":["Mount your drive in order to run locally with colab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"IpNBHS0MTggD"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qg89LL10TfQw"},"source":["download & load the MNIST dataset.\n","\n","*just run the next two cells and observe the outputs (shift&enter) "]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{},"colab_type":"code","id":"0oFFceCNGgMo"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading train-images-idx3-ubyte.gz ... \n","Done\n","Downloading train-labels-idx1-ubyte.gz ... \n","Done\n","Downloading t10k-images-idx3-ubyte.gz ... \n","Done\n","Downloading t10k-labels-idx1-ubyte.gz ... \n","Done\n","Converting train-images-idx3-ubyte.gz to NumPy Array ...\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/train-images-idx3-ubyte.gz'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-3bb0864418be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-3bb0864418be>\u001b[0m in \u001b[0;36mload_mnist\u001b[0;34m(normalize, flatten, one_hot_label)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0minit_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-3bb0864418be>\u001b[0m in \u001b[0;36minit_mnist\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdownload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating pickle file ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-3bb0864418be>\u001b[0m in \u001b[0;36m_convert_numpy\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_convert_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_img'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0m_load_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_img'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-3bb0864418be>\u001b[0m in \u001b[0;36m_load_img\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Converting \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" to NumPy Array ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/cvapps/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/cvapps/lib/python3.9/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train-images-idx3-ubyte.gz'"]}],"source":["\n","#importing modules that will be in use\n","%matplotlib inline\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import urllib.request\n","import gzip\n","import pickle\n","from PIL import Image\n","import random\n","import numpy as np\n","\n","def _download(file_name):\n","    file_path = os.path.join(dataset_dir,file_name)\n","\n","    if os.path.exists(file_path):\n","        return\n","\n","    print(\"Downloading \" + file_name + \" ... \")\n","    urllib.request.urlretrieve(url_base + file_name, file_name)\n","    print(\"Done\")\n","\n","def download_mnist():\n","    for v in key_file.values():\n","       _download(v)\n","\n","def _load_label(file_name):\n","    file_path =  os.path.join(dataset_dir, file_name)\n","\n","    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n","    with gzip.open(file_path, 'rb') as f:\n","            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n","    print(\"Done\")\n","\n","    return labels\n","\n","def _load_img(file_name):\n","    file_path = os.path.join(dataset_dir,file_name)\n","\n","    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n","    with gzip.open(file_path, 'rb') as f:\n","            data = np.frombuffer(f.read(), np.uint8, offset=16)\n","    data = data.reshape(-1, img_size)\n","    print(\"Done\")\n","\n","    return data\n","\n","def _convert_numpy():\n","    dataset = {}\n","    dataset['train_img'] =  _load_img(key_file['train_img'])\n","    dataset['train_label'] = _load_label(key_file['train_label'])\n","    dataset['test_img'] = _load_img(key_file['test_img'])\n","    dataset['test_label'] = _load_label(key_file['test_label'])\n","\n","    return dataset\n","\n","def init_mnist():\n","    download_mnist()\n","    dataset = _convert_numpy()\n","    print(\"Creating pickle file ...\")\n","    with open(save_file, 'wb') as f:\n","        pickle.dump(dataset, f, -1)\n","    print(\"Done\")\n","\n","def _change_one_hot_label(X):\n","    T = np.zeros((X.size, 10))\n","    for idx, row in enumerate(T):\n","        row[X[idx]] = 1\n","\n","    return T\n","\n","def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    normalize : Normalize the pixel values\n","    flatten : Flatten the images as one array\n","    one_hot_label : Encode the labels as a one-hot array\n","\n","    Returns\n","    -------\n","    (Trainig Image, Training Label), (Test Image, Test Label)\n","    \"\"\"\n","    if not os.path.exists(save_file):\n","        init_mnist()\n","\n","    with open(save_file, 'rb') as f:\n","        dataset = pickle.load(f)\n","\n","    if normalize:\n","        for key in ('train_img', 'test_img'):\n","            dataset[key] = dataset[key].astype(np.float32)\n","            dataset[key] /= 255.0\n","\n","    if not flatten:\n","         for key in ('train_img', 'test_img'):\n","            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n","\n","    if one_hot_label:\n","        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n","        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n","\n","    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label'])\n","\n","\n","# Load the MNIST dataset\n","url_base = 'http://yann.lecun.com/exdb/mnist/'\n","key_file = {\n","    'train_img':'train-images-idx3-ubyte.gz',\n","    'train_label':'train-labels-idx1-ubyte.gz',\n","    'test_img':'t10k-images-idx3-ubyte.gz',\n","    'test_label':'t10k-labels-idx1-ubyte.gz'\n","}\n","\n","dataset_dir = '/content'\n","save_file = dataset_dir + \"/mnist.pkl\"\n","\n","train_num = 60000\n","test_num = 10000\n","img_dim = (1, 28, 28)\n","img_size = 784\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True)\n","\n","\n","# printing data shape\n","\n","print('the training data set contains '+ str(x_train.shape[0]) + ' samples')\n","\n","img = x_train[0]\n","label = t_train[0]\n","\n","img = img.reshape(28, 28)\n","print('each sample image from the training data set is a column-stacked grayscale image of '+ str(x_train.shape[1]) +' pixels'\n","      + '\\n this vectorized arrangement of the data is suitable for a Fully-Connected NN (as apposed to a Convolutional NN)' )\n","print('these column-stacked images can be reshaped to an image of ' +str(img.shape)+ ' pixels')\n","\n","# printing a sample from the dataset\n","\n","plt.imshow(img, cmap='gray')\n","plt.axis('off')\n","plt.title('The ground truth label of this image is '+str(label))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"Q8h43VmTIFCo"},"outputs":[],"source":["# Visualize some examples from the dataset.\n","# We'll show a few examples of training images from each class.\n","num_classes = 10\n","samples_per_class = 7\n","for cls in range(num_classes):\n","    idxs = np.argwhere(t_train==cls)\n","    sample = np.random.choice(idxs.shape[0], samples_per_class, replace=False) # randomly picks 7 from the appearences \n","    idxs=idxs[sample]\n","\n","    for i, idx in enumerate(idxs):\n","        plt_idx = i * num_classes + cls + 1\n","        plt.subplot(samples_per_class, num_classes, plt_idx)\n","        img = x_train[idx].reshape(28, 28)\n","\n","        plt.imshow(img, cmap='gray')\n","        plt.axis('off')\n","        if i == 0:\n","            plt.title(cls)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R3y5IDyqIFCr"},"source":["**QUESTION 1**:What are vanishing gradients? Name one known activation function that has this problem and one that does not.\n","\n","**ANSWER**:\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dFMPV9CxIFCs"},"source":["here we will implement the sigmoid activation function and it's gradient "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"1IP_aR9bIFCt"},"outputs":[],"source":["\n","def sigmoid(x):\n","    #############################################################################\n","    #                             YOUR CODE                                     #\n","    #############################################################################   \n","\n","    \n","    ############################################################################\n","    #                             END OF YOUR CODE                             #\n","    ############################################################################\n","    return sig\n","def sigmoid_grad(x):\n","    #############################################################################\n","    #                             YOUR CODE                                     #\n","    #############################################################################   \n","\n","    \n","    ############################################################################\n","    #                             END OF YOUR CODE                             #\n","    ############################################################################\n","    return sig_grad\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nVYLu-cOIFCx"},"source":["Implement a fully-vectorized loss function for the Softmax classifier\n","Make sure the softmax is stable.\n","To make our softmax function numerically stable,we simply normalize the values in the vector, \n","by multiplying the numerator and denominator with a constant C.\n","We can choose an arbitrary value for log(C) term, but generally log(C)=−max(a) is chosen, as it shifts all of elements in the vector to negative to zero, and negatives with large exponents saturate to zero rather than the infinity."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"Mp4q0S7yIFCy"},"outputs":[],"source":["def softmax(x):\n","    \"\"\"\n","  Softmax loss function, should be implemented in a vectorized fashion (without loops)\n","\n","\n","  Inputs:\n","  - X: A numpy array of shape (N, C) containing a minibatch of data.\n","  Returns:\n","  - probabilities: A numpy array of shape (N, C) containing the softmax probabilities.\n","\n","  if you are not careful here, it is easy to run into numeric instability\n","     \"\"\"\n","    #############################################################################\n","    #                             YOUR CODE                                     #\n","    #############################################################################   \n"," \n","\n","    ############################################################################\n","    #                             END OF YOUR CODE                             #\n","    ############################################################################\n","    return probabilities\n","\n","def cross_entropy_error(y, t):\n","    \"\"\"\n","    Inputs:\n","\n","    - t:  A numpy array of shape (N,C) containing  a minibatch of training labels, it is a one-hot array, \n","      with t[GT]=1 and t=0 elsewhere, where GT is the ground truth label ; \n","    - y: A numpy array of shape (N, C) containing the softmax probabilities (the NN's output).\n","\n","    Returns a tuple of:\n","    - loss as single float (do not forget to divide by the number of samples in the minibatch (N))\n","    \"\"\"\n","    #############################################################################\n","    #                             YOUR CODE                                     #\n","    #############################################################################   \n","    # Compute loss \n","\n","\n","    ############################################################################\n","    #                             END OF YOUR CODE                             #\n","    ############################################################################\n","    return error"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RCu1s4BLIFC1"},"source":["We will design and train a two-layer fully-connected neural network with sigmoid nonlinearity and\n","softmax cross entropy loss. We assume an input dimension of D=784, a hidden dimension of H, and perform classification over C classes.\n","\n","The architecture should be fullyconnected -> sigmoid -> fullyconnected -> softmax.\n","\n","The learnable parameters of the model are stored in the dictionary,\n","'params', that maps parameter names to numpy arrays.\n","\n","In the next cell we will initialize the weights and biases, design the fully connected(fc) forward and backward functions that will be in use for the training (using SGD).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"BOs-KahYIFC2"},"outputs":[],"source":["def TwoLayerNet( input_size, hidden_size, output_size, weight_init_std=0.01):\n","    ############################################################################\n","    # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n","    # should be initialized from a Gaussian with standard deviation equal to   #\n","    # weight_init_std, and biases should be initialized to zero. All weights and  #\n","    # biases should be stored in the dictionary 'params', with first layer  #\n","    # weights and biases using the keys 'W1' and 'b1' and second layer weights #\n","    # and biases using the keys 'W2' and 'b2'.                                 #\n","    ############################################################################\n","    \n","    \n","\n","    ############################################################################\n","    #                             END OF YOUR CODE                             #\n","    ############################################################################\n","    return params\n","\n","\n","def FC_forward(x, w, b):\n","    \"\"\"\n","    Computes the forward pass for a fully-connected layer.\n","    The input x has shape (N, D) and contains a minibatch of N\n","    examples, where each example x[i] has shape D and will be transformed to an output vector of dimension M.\n","    Inputs:\n","    - x: A numpy array containing input data, of shape (N, D)\n","    - w: A numpy array of weights, of shape (D, M)\n","    - b: A numpy array of biases, of shape (M,)\n","\n","    Returns a tuple of:\n","    - out: output result of the forward pass, of shape (N, M)\n","    - cache: (x, w, b)\n","    \"\"\"\n","    out = None\n","    #############################################################################\n","    #                             YOUR CODE                                     #\n","    #############################################################################   \n","\n","    #############################################################################\n","    #                             END OF YOUR CODE                              #\n","    #############################################################################\n","    cache = (x, w, b)\n","    return out, cache\n","\n","\n","\n","def FC_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a fully-connected layer.\n","    Inputs:\n","    - dout: Upstream derivative, of shape (N, M)\n","    - cache: Tuple of:\n","    - w: Weights, of shape (D, M)\n","    Returns a tuple of:\n","    - dx: Gradient with respect to x, of shape (N, D)\n","    - dw: Gradient with respect to w, of shape (D, M)\n","    - db: Gradient with respect to b, of shape (M,)\n","    \"\"\"\n","    x, w, b = cache\n","    dx, dw, db = None, None, None\n","    \n","    #############################################################################\n","    #                             YOUR CODE                                     #\n","    #############################################################################   \n","    \n","    \n","    #############################################################################\n","    #                             END OF YOUR CODE                              #\n","    #############################################################################\n","    return dx, dw, db\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ttgcf5YeIFC5"},"source":["Here we will design the entire model, which outputs the NN's probabilities and gradients.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"07syYZaYIFC6"},"outputs":[],"source":["def Model(params, x, t):\n","    \"\"\"\n","    Computes the backward pass for a fully-connected layer.\n","    Inputs:\n","    - params:  dictionary with first layer weights and biases using the keys 'W1' and 'b1' and second layer weights\n","    and biases using the keys 'W2' and 'b2'. each with dimensions corresponding its input and output dimensions.  \n","    - x: Input data, of shape (N,D)\n","    - t:  A numpy array of shape (N,C) containing training labels, it is a one-hot array, \n","      with t[GT]=1 and t=0 elsewhere, where GT is the ground truth label ; \n","    Returns:\n","    - y: the output probabilities for the minibatch (at the end of the forward pass) of shape (N,C)\n","    - grads: dictionary containing gradients of the loss with respect to W1, W2, b1, b2.\n","    \n","    note: use the FC_forward ,FC_backward functions.\n","\n","    \"\"\"\n","    W1, W2 = params['W1'], params['W2']\n","    b1, b2 = params['b1'], params['b2']\n","    grads = {'W1': None ,'W2': None, 'b1': None ,'b2': None }\n","\n","    batch_num = x.shape[0]\n","\n","    #############################################################################\n","    #                             YOUR CODE                                     #\n","    #############################################################################   \n","    # forward (fullyconnected -> sigmoid -> fullyconnected -> softmax).\n","\n","    \n","    # backward - calculate gradients.\n","    \n","    \n","    #############################################################################\n","    #                             END OF YOUR CODE                              #\n","    #############################################################################\n","    \n","    return grads, y"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YPDLXpWcIFC9"},"source":["Compute the accuracy of the NNs predictions.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"AS-mv6s3IFC9"},"outputs":[],"source":["\n","\n","def accuracy(y,t):\n","    \"\"\"\n","    Computes the accuracy of the NN's predictions.\n","    Inputs:\n","    - t:  A numpy array of shape (N,C) containing training labels, it is a one-hot array, \n","      with t[GT]=1 and t=0 elsewhere, where GT is the ground truth label ; \n","    - y: the output probabilities for the minibatch (at the end of the forward pass) of shape (N,C)\n","    Returns:\n","    - accuracy: a single float of the average accuracy.\n","    \"\"\"\n","    #############################################################################\n","    #                             YOUR CODE                                     #\n","    #############################################################################   \n","    \n","    \n","    #############################################################################\n","    #                             END OF YOUR CODE                              #\n","    #############################################################################    \n","    return accuracy\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nwJJtrHoIFDB"},"source":["Trianing the model:\n","To train our network we will use minibatch SGD.  \n","*Note that the test dataset is actually used as the validation dataset in the training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"B4Fvu0jfIFDC"},"outputs":[],"source":["\n","\n","# You should be able to receive at least 97% accuracy, choose hyperparameters accordingly.\n","\n","epochs =\n","mini_batch_size =\n","learning_rate = \n","num_hidden_cells =\n","\n","def Train(epochs_num, batch_size, lr, H):\n","    #  Dividing a dataset into training data and test data\n","\n","    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","    C=10\n","    D=x_train.shape[1]\n","    network_params = TwoLayerNet(input_size=D, hidden_size=H, output_size=C) #hidden_size is the only hyperparameter here\n","\n","    train_size = x_train.shape[0]\n","    train_loss_list = []\n","    train_acc_list = []\n","    test_acc_list = []\n","    iter_per_epoch = round(train_size / batch_size)\n","    \n","    print('training of ' + str(epochs_num) +' epochs, each epoch will have '+ str(iter_per_epoch)+ ' iterations')\n","    for i in range(epochs_num):\n","    \n","        train_loss_iter= []\n","        train_acc_iter= []\n","\n","        for k in range(iter_per_epoch):\n","            \n","            \n","            #############################################################################\n","            #                             YOUR CODE                                     #\n","            #############################################################################               \n","            # 1. Select part of training data (mini-batch) randomly\n","           \n","\n","            # 2. Calculate the predictions and the gradients to reduce the value of the loss function\n","\n","            \n","            # 3. Update weights and biases with the gradients \n","            \n","            \n","            #############################################################################\n","            #                             END OF YOUR CODE                              #\n","            #############################################################################            \n","        \n","            # Calculate the loss and accuracy for visalizaton \n","\n","            error=cross_entropy_error(y_batch, t_batch)\n","            train_loss_iter.append(error)\n","            acc_iter=accuracy(y_batch, t_batch)\n","            train_acc_iter.append(acc_iter)\n","            if k == iter_per_epoch-1:\n","                train_acc = np.mean(train_acc_iter)\n","                train_acc_list.append(train_acc)\n","                train_loss_list.append(np.mean(train_loss_iter))\n","\n","                _, y_test = Model(network_params, x_test, t_test)\n","                test_acc = accuracy(y_test, t_test)\n","                test_acc_list.append(test_acc)\n","                print(\"train acc: \" + str(train_acc)[:5] + \"% |  test acc: \"   + str(test_acc) + \"% |  loss for epoch \" + str(i) +\": \"+ str(np.mean(train_loss_iter)))\n","    return train_acc_list, test_acc_list, train_loss_list, network_params\n","\n","train_acc, test_acc, train_loss, net_params = Train(epochs, mini_batch_size, learning_rate, num_hidden_cells)\n","\n","markers = {'train': 'o', 'test': 's'}\n","x = np.arange(len(train_acc))\n","plt.plot(x, train_acc, label='train acc')\n","plt.plot(x, test_acc, label='test acc', linestyle='--')\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.legend(loc='lower right')\n","plt.show()\n","\n","\n","markers = {'train': 'o'}\n","x = np.arange(len(train_loss))\n","plt.plot(x, train_loss, label='train loss')\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend(loc='lower right')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5E2I2GpOIFDH"},"source":["# You should be able to receive at least 97% accuracy, choose hyperparameters accordingly.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R81tqW63IFDI"},"source":["**QUESTION 2:** Explain the results looking at the visualizations above, base your answer on the hyperparameters.\n","\n","**ANSWER:**\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"c62bjGf8IFDK"},"source":["**QUESTION** 3: Suggest a way to improve the results by changing the networks's architecture  \n","\n","**ANSWER**:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"BvstDmGeIFDL"},"outputs":[],"source":["# Visualize some weights. features of digits should be somehow present.\n","def show_net_weights(params):\n","    W1 = params['W1']\n","    print(W1.shape)\n","    for i in range(5):\n","        W = W1[:,i*5].reshape(28, 28)\n","        plt.imshow(W,cmap='gray')\n","        plt.axis('off')\n","        plt.show()\n","\n","show_net_weights(net_params)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LWE9-3qSOzBJ"},"source":["Implement, train and test the same two-layer network, using a **deep learning library** (pytorch/tensorflow/keras).\n","\n","As before, you should be able to receive at least 97% accuracy.\n","\n","Please note, that in this section you will need to implement the model, the training and the testing by yourself (you may use the code in earlier sections)\n","Don't forget to print the accuracy during training (in the same format as before).\n","\n","For installing a deep learning library, you should use \"!pip3 install...\" (lookup the compatible syntex for your library)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"UU5fdOGFNcMO"},"outputs":[],"source":[" #############################################################################\n"," #                             YOUR CODE                                     #\n"," #############################################################################           "]}],"metadata":{"colab":{"collapsed_sections":[],"name":"CV_mnist_net.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}
